{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "QA_bi_cross_encoder_gradio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e47ee1a-4f1c-4a4b-8860-9483cfef884c"
      },
      "source": [
        "# Question-Answering using Simple Wikipedia Index\n",
        "\n",
        "**This script uses the smaller Simple English Wikipedia as document collection to provide answers to user questions / search queries**. This examples demonstrates the setup for Query / Question-Answer-Retrieval. First, we split all Wikipedia articles into paragraphs and encode them with a bi-encoder *(Min comment: get sentence embedding)*. If a new query / question is entered, it is encoded by the same bi-encoder and the paragraphs with the highest cosine-similarity are retrieved (see semantic search). Next, the retrieved candidates are scored by a Cross-Encoder re-ranker and the 5 passages with the highest score from the Cross-Encoder are presented to the user.\n",
        "\n",
        "https://colab.research.google.com/drive/1l6stpYdRMmeDBK_vw0L5NitdiAuhdsAr?usp=sharing\n",
        "\n",
        "You can input a query or a question. The script then uses semantic search to find relevant passages in Simple English Wikipedia (as it is smaller and fits better in RAM).\n",
        "\n",
        "For semantic search, we use SentenceTransformer('msmarco-distilbert-base-v2') and retrieve 100 potentially passages that answer the input query.\n",
        "\n",
        "Next, we use a more powerful CrossEncoder (cross_encoder = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-6')) that\n",
        "scores the query and all retrieved passages for their relevancy. The cross-encoder is neccessary to filter out certain noise\n",
        "that might be retrieved from the semantic search step."
      ],
      "id": "3e47ee1a-4f1c-4a4b-8860-9483cfef884c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26d33316-8616-4916-be09-70579bfcaf13"
      },
      "source": [
        "import pandas as pd"
      ],
      "id": "26d33316-8616-4916-be09-70579bfcaf13",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv4S83iSn1fc"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "id": "Hv4S83iSn1fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00981a46-a10e-4157-9e35-672c925d7db5"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning: No GPU found. Please add GPU to your notebook\")"
      ],
      "id": "00981a46-a10e-4157-9e35-672c925d7db5",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP3Ky3afx8uW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc7d1c8-6577-482e-b91a-ba00939daab3"
      },
      "source": [
        "!pip install rank-bm25"
      ],
      "id": "VP3Ky3afx8uW",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.1-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank-bm25) (1.19.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9yMLL9SxzZQ"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "#from sklearn.feature_extraction import stop_words\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np"
      ],
      "id": "-9yMLL9SxzZQ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0zKgPuicqYN"
      },
      "source": [
        "# Checking the Data"
      ],
      "id": "K0zKgPuicqYN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Xa-eYUcYpV"
      },
      "source": [
        "embeddings_filepath = '/content/close_defects_small.csv'  # D:/python_working_dir/nlp/data/open_dmg_9.csv\n",
        "\n",
        "df = pd.read_csv(embeddings_filepath, encoding = 'utf-8', sep = \";\")\n",
        "df.head()\n",
        "\n",
        "# filling nan\n",
        "df[['Skadebeskrivning', 'Skaderubrik', 'Åtgärdsbeskrivning']] = df[['Skadebeskrivning','Skaderubrik', 'Åtgärdsbeskrivning']].fillna(value='')\n",
        "\n",
        "df['Skade_text'] = df['Skaderubrik'] + ' ' + df['Skadebeskrivning']\n",
        "\n",
        "# droping nan \n",
        "df = df[df['Skade_text'].notna()]\n",
        "\n",
        "passages = df['Skade_text'].values.tolist()\n",
        "len(passages)\n",
        "#passage\n",
        "df.head()"
      ],
      "id": "x-Xa-eYUcYpV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a079b76-97ee-474a-9966-f32e7f7c4906"
      },
      "source": [
        "# Empty Memeory of GPU"
      ],
      "id": "3a079b76-97ee-474a-9966-f32e7f7c4906"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a3e4d8-7dcb-44fb-8cc6-3ce191344bdd"
      },
      "source": [
        "# empty the memeory of gpu\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "id": "75a3e4d8-7dcb-44fb-8cc6-3ce191344bdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e219e42f-61a1-402b-8087-e38dc6fa6f30"
      },
      "source": [
        "# Custom Function to Run Multiple Models "
      ],
      "id": "e219e42f-61a1-402b-8087-e38dc6fa6f30"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5458e330-636d-47c3-83ee-41c51466cd04"
      },
      "source": [
        "#This function will search all texts in passages that answer the query\n",
        "def model_search(query, bi_encoder_name, cross_encoder_name, top_k_biencoder, top_n_res):\n",
        "    \n",
        "    bi_encoder = SentenceTransformer(bi_encoder_name)\n",
        "    top_k = top_k_biencoder     #Number of passages we want to retrieve with the bi-encoder\n",
        "\n",
        "    #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
        "    cross_encoder = CrossEncoder(cross_encoder_name)\n",
        "    \n",
        "    #Here, we compute the corpus_embeddings from scratch (which can take a while depending on the GPU)\n",
        "    corpus_embeddings = bi_encoder.encode(passages,  batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "    \n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    #BM25 search (lexical search)\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -top_n_res)[-top_n_res:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "    print(\"Top-\" + str(top_n_res) + \"lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_n_res]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "\n",
        "    ##### Sematic Search #####\n",
        "    #Encode the query using the bi-encoder and find potentially relevant passages\n",
        "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "    question_embedding = question_embedding.cuda()\n",
        "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
        "    hits = hits[0]  # Get the hits for the first query\n",
        "\n",
        "    ##### Re-Ranking #####\n",
        "    #Now, score all retrieved passages with the cross_encoder\n",
        "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "\n",
        "    #Sort results by the cross-encoder scores\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx]['cross-score'] = cross_scores[idx]\n",
        "\n",
        "\n",
        "    #Output of top-10 hits\n",
        "    print(\"Top-\" + str(top_n_res) + \"Bi-Encoder Retrieval hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "    for hit in hits[0:top_n_res]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "        print(hit['corpus_id'])\n",
        "\n",
        "    print(\"Top-\" + str(top_n_res) + \"Cross-Encoder Re-ranker hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    for hit in hits[0:top_n_res]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
      ],
      "id": "5458e330-636d-47c3-83ee-41c51466cd04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fd9ad6-2564-4cb5-b0e5-c68d85628cf6"
      },
      "source": [
        "bi_encoder_model_list = [\"paraphrase-multilingual-mpnet-base-v2\", \"sentence-transformers/stsb-xlm-r-multilingual\", \n",
        "                         \"sentence-transformers/msmarco-distilbert-multilingual-en-de-v2-tmp-lng-aligned\"]\n",
        "cross_encoder_model_list = [\"cross-encoder/ms-marco-TinyBERT-L-6\", \n",
        "                            \"cross-encoder/quora-roberta-large\", \n",
        "                            \"cross-encoder/qnli-electra-base\",\n",
        "                           \"cross-encoder/stsb-roberta-large\"]\n",
        "\n",
        "top_k = 70     #Number of passages we want to retrieve with the bi-encoder\n",
        "query = \"Vilka fel ligger på vagn 2\"  # questions: Vilka fel på vagn a2\n",
        "\n",
        "model_search(query, bi_encoder_name = bi_encoder_model_list[0], cross_encoder_name = cross_encoder_model_list[0], \n",
        "             top_k_biencoder=top_k, top_n_res=10)\n",
        "# memery issue, cannot run the loops\n",
        "[model_search(query, bi_encoder_name, cross_encoder_name, top_k_biencoder=top_k, top_n_res=10) \n",
        " for bi_encoder_name in bi_encoder_model_list for cross_encoder_name in cross_encoder_model_list]        "
      ],
      "id": "41fd9ad6-2564-4cb5-b0e5-c68d85628cf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ae362d-847f-41d0-906d-69d05c353233"
      },
      "source": [
        "# \"cross-encoder/quora-roberta-large\" : better\n",
        "model_search(query, bi_encoder_name = bi_encoder_model_list[0], cross_encoder_name = cross_encoder_model_list[1], \n",
        "             top_k_biencoder=top_k, top_n_res=10)"
      ],
      "id": "18ae362d-847f-41d0-906d-69d05c353233",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0e5d280-3d38-4331-ba35-263f71524f68"
      },
      "source": [
        "bi_encoder_model_list = [\"paraphrase-multilingual-mpnet-base-v2\", \"sentence-transformers/stsb-xlm-r-multilingual\", \n",
        "                         \"sentence-transformers/msmarco-distilbert-multilingual-en-de-v2-tmp-lng-aligned\"]\n",
        "cross_encoder_model_list = [\"cross-encoder/ms-marco-TinyBERT-L-6\", \n",
        "                            \"cross-encoder/quora-roberta-large\", \n",
        "                            \"cross-encoder/qnli-electra-base\",\n",
        "                           \"cross-encoder/stsb-roberta-large\"]\n",
        "\n",
        "# bi_encoder = sentence-transformers/stsb-xlm-r-multilingual, good\n",
        "model_search(query, bi_encoder_name = bi_encoder_model_list[1], cross_encoder_name = cross_encoder_model_list[2], \n",
        "             top_k_biencoder=top_k, top_n_res=10)"
      ],
      "id": "f0e5d280-3d38-4331-ba35-263f71524f68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA4I-nRca1bo"
      },
      "source": [
        "# gradio"
      ],
      "id": "ZA4I-nRca1bo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy2iYQjHm-t6"
      },
      "source": [
        "## Fixed Models"
      ],
      "id": "uy2iYQjHm-t6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xQX6Tu7nBJY"
      },
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import torch\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "   #from sklearn.feature_extraction import stop_words\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "  tokenized_doc = []\n",
        "  for token in text.lower().split():\n",
        "    token = token.strip(string.punctuation)\n",
        "\n",
        "    #if len(token) > 0 and token not in stop_words.ENGLISH_STOP_WORDS:\n",
        "    tokenized_doc.append(token)\n",
        "  return tokenized_doc\n",
        "\n",
        "#This function will search all texts in passages that answer the query\n",
        "def model_search(query):\n",
        "  \n",
        "  embeddings_filepath = '/content/close_defects_small.csv'  # D:/python_working_dir/nlp/data/open_dmg_9.csv\n",
        "  df = pd.read_csv(embeddings_filepath, encoding = 'utf-8', sep = \";\")\n",
        "  # filling nan\n",
        "  df[['Skadebeskrivning', 'Skaderubrik', 'Åtgärdsbeskrivning']] = df[['Skadebeskrivning','Skaderubrik', 'Åtgärdsbeskrivning']].fillna(value='')\n",
        "  df['Skade_text'] = df['Skaderubrik'] + ' ' + df['Skadebeskrivning']\n",
        "  # droping nan\n",
        "  df = df[df['Skade_text'].notna()]\n",
        "  passages = df['Skade_text'].values.tolist()\n",
        "    \n",
        "  bi_encoder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "  top_k = 100     #Number of passages we want to retrieve with the bi-encoder\n",
        "\n",
        "  #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
        "  cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\")\n",
        "    \n",
        "  #Here, we compute the corpus_embeddings from scratch (which can take a while depending on the GPU)\n",
        "  corpus_embeddings = bi_encoder.encode(passages,  batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "    \n",
        "  #print(\"Input question:\", query)\n",
        "\n",
        "  #BM25 search (lexical search)\n",
        "\n",
        "  tokenized_corpus = []\n",
        "  for passage in tqdm(passages):\n",
        "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
        "\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "  bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "  top_n = np.argpartition(bm25_scores, -10)[-10:]\n",
        "  bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "  bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "  bm25_output = []\n",
        "  print(\"Top-\" + str(10) + \"lexical search (BM25) hits\")\n",
        "  for hit in bm25_hits[0:10]:\n",
        "    line = str(round(hit['score'], 2)) + \" , \" + passages[hit['corpus_id']]\n",
        "    bm25_output.append(line)\n",
        "\n",
        "  ##### Sematic Search #####\n",
        "  #Encode the query using the bi-encoder and find potentially relevant passages\n",
        "  question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "  question_embedding = question_embedding.cuda()\n",
        "  hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
        "  hits = hits[0]  # Get the hits for the first query\n",
        "\n",
        "  ##### Re-Ranking #####\n",
        "  #Now, score all retrieved passages with the cross_encoder\n",
        "  cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
        "  cross_scores = cross_encoder.predict(cross_inp)\n",
        "\n",
        "  #Sort results by the cross-encoder scores\n",
        "  for idx in range(len(cross_scores)):\n",
        "    hits[idx]['cross-score'] = cross_scores[idx]\n",
        "\n",
        "\n",
        "  #Output of top-10 hits\n",
        "  print(\"Top-\" + str(10) + \"Bi-Encoder Retrieval hits\")\n",
        "  hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "  bi_encoder_output = []\n",
        "  for hit in hits[0:10]:\n",
        "    line_bi = str(round(hit['score'], 2)) + \" , \" + passages[hit['corpus_id']] + \" . \" + \"Åtgärder: \" + df.Åtgärdsbeskrivning[hit['corpus_id']] + \"\\n  \"\n",
        "    bi_encoder_output.append(line_bi)\n",
        "\n",
        "  print(\"Top-\" + str(10) + \"Cross-Encoder Re-ranker hits\")\n",
        "  hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "  cross_encoder_output = []\n",
        "  for hit in hits[0:10]:\n",
        "    line_c = str(round(hit['cross-score'], 2)) + \" , \" + passages[hit['corpus_id']]  + \" . \" + \"Åtgärder: \" + df.Åtgärdsbeskrivning[hit['corpus_id']] + \"\\n  \"\n",
        "    cross_encoder_output.append(line_c)\n",
        "  \n",
        "  return bm25_output, bi_encoder_output, cross_encoder_output\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=model_search,\n",
        "  inputs=[\"text\"], \n",
        "  outputs=[\"text\", \"text\", \"text\"])\n",
        "iface.launch(debug=True)\n"
      ],
      "id": "6xQX6Tu7nBJY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLxJTS0jprTx"
      },
      "source": [
        "## Non-fixed Models"
      ],
      "id": "DLxJTS0jprTx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtbLy-smh-2y"
      },
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import torch\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "   #from sklearn.feature_extraction import stop_words\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "  tokenized_doc = []\n",
        "  for token in text.lower().split():\n",
        "    token = token.strip(string.punctuation)\n",
        "\n",
        "    #if len(token) > 0 and token not in stop_words.ENGLISH_STOP_WORDS:\n",
        "    tokenized_doc.append(token)\n",
        "  return tokenized_doc\n",
        "\n",
        "#This function will search all texts in passages that answer the query\n",
        "def model_search(query, bi_encoder_name, cross_encoder_name, top_k_biencoder, top_n_res):\n",
        "    \n",
        "    embeddings_filepath = '/content/close_defects_small.csv'  # D:/python_working_dir/nlp/data/open_dmg_9.csv\n",
        "    df = pd.read_csv(embeddings_filepath, encoding = 'utf-8', sep = \";\")\n",
        "    # filling nan\n",
        "    df[['Skadebeskrivning', 'Skaderubrik']] = df[['Skadebeskrivning','Skaderubrik']].fillna(value='')\n",
        "    df['Skade_text'] = df['Skaderubrik'] + ' ' + df['Skadebeskrivning']\n",
        "    # droping nan\n",
        "    df = df[df['Skade_text'].notna()]\n",
        "    passages = df['Skade_text'].values.tolist()\n",
        "\n",
        "    bi_encoder = SentenceTransformer(bi_encoder_name)\n",
        "    top_k = int(top_k_biencoder)     #Number of passages we want to retrieve with the bi-encoder\n",
        "\n",
        "    #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
        "    cross_encoder = CrossEncoder(cross_encoder_name)\n",
        "    \n",
        "    #Here, we compute the corpus_embeddings from scratch (which can take a while depending on the GPU)\n",
        "    corpus_embeddings = bi_encoder.encode(passages,  batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "    \n",
        "    #BM25 search (lexical search)\n",
        "    tokenized_corpus = []\n",
        "    for passage in tqdm(passages):\n",
        "      tokenized_corpus.append(bm25_tokenizer(passage))\n",
        "\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n_res = int(top_n_res)\n",
        "    top_n = np.argpartition(bm25_scores, -top_n_res)[-top_n_res:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    print(\"Top-\" + str(top_n_res) + \"lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_n_res]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "\n",
        "    ##### Sematic Search #####\n",
        "    #Encode the query using the bi-encoder and find potentially relevant passages\n",
        "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "    question_embedding = question_embedding.cuda()\n",
        "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
        "    hits = hits[0]  # Get the hits for the first query\n",
        "\n",
        "    ##### Re-Ranking #####\n",
        "    #Now, score all retrieved passages with the cross_encoder\n",
        "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "\n",
        "    #Sort results by the cross-encoder scores\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx]['cross-score'] = cross_scores[idx]\n",
        "\n",
        "\n",
        "    #Output of top-10 hits\n",
        "    print(\"Top-\" + str(top_n_res) + \"Bi-Encoder Retrieval hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "    for hit in hits[0:top_n_res]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "        print(hit['corpus_id'])\n",
        "    \n",
        "    print(\"Top-\" + str(top_n_res) + \"Cross-Encoder Re-ranker hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    for hit in hits[0:top_n_res]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "    \n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_search,\n",
        "    inputs=[\"text\", \"text\", \"text\", \"number\", \"number\"], \n",
        "    outputs=[\"text\"])\n",
        "iface.launch(debug=True)"
      ],
      "id": "ZtbLy-smh-2y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek44J7tI6tBu"
      },
      "source": [
        "### Return"
      ],
      "id": "ek44J7tI6tBu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_qP29AH6rgX"
      },
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import torch\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "   #from sklearn.feature_extraction import stop_words\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "  tokenized_doc = []\n",
        "  for token in text.lower().split():\n",
        "    token = token.strip(string.punctuation)\n",
        "\n",
        "    #if len(token) > 0 and token not in stop_words.ENGLISH_STOP_WORDS:\n",
        "    tokenized_doc.append(token)\n",
        "  return tokenized_doc\n",
        "\n",
        "#This function will search all texts in passages that answer the query\n",
        "def model_search(query, bi_encoder_name, cross_encoder_name, top_k_biencoder, top_n_res):\n",
        "    \n",
        "    embeddings_filepath = '/content/close_defects_small.csv'  # D:/python_working_dir/nlp/data/open_dmg_9.csv\n",
        "    df = pd.read_csv(embeddings_filepath, encoding = 'utf-8', sep = \";\")\n",
        "    # filling nan\n",
        "    df[['Skadebeskrivning', 'Skaderubrik', 'Åtgärdsbeskrivning']] = df[['Skadebeskrivning','Skaderubrik', 'Åtgärdsbeskrivning']].fillna(value='')\n",
        "    df['Skade_text'] = df['Skaderubrik'] + ' ' + df['Skadebeskrivning']\n",
        "    # droping nan\n",
        "    df = df[df['Skade_text'].notna()]\n",
        "    passages = df['Skade_text'].values.tolist()\n",
        "\n",
        "    bi_encoder = SentenceTransformer(bi_encoder_name)\n",
        "    top_k = int(top_k_biencoder)     #Number of passages we want to retrieve with the bi-encoder\n",
        "\n",
        "    #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
        "    cross_encoder = CrossEncoder(cross_encoder_name)\n",
        "    \n",
        "    #Here, we compute the corpus_embeddings from scratch (which can take a while depending on the GPU)\n",
        "    corpus_embeddings = bi_encoder.encode(passages,  batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "    \n",
        "    #BM25 search (lexical search)\n",
        "    tokenized_corpus = []\n",
        "    for passage in tqdm(passages):\n",
        "      tokenized_corpus.append(bm25_tokenizer(passage))\n",
        "\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n_res = int(top_n_res)\n",
        "    top_n = np.argpartition(bm25_scores, -top_n_res)[-top_n_res:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    \n",
        "    bm25_output = []\n",
        "\n",
        "    print(\"Top-\" + str(top_n_res) + \"lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_n_res]:\n",
        "        line = str(round(hit['score'], 2)) + \" , \" + passages[hit['corpus_id']]\n",
        "        bm25_output.append(line)\n",
        "\n",
        "    ##### Sematic Search #####\n",
        "    #Encode the query using the bi-encoder and find potentially relevant passages\n",
        "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "    question_embedding = question_embedding.cuda()\n",
        "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
        "    hits = hits[0]  # Get the hits for the first query\n",
        "\n",
        "    ##### Re-Ranking #####\n",
        "    #Now, score all retrieved passages with the cross_encoder\n",
        "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "\n",
        "    #Sort results by the cross-encoder scores\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx]['cross-score'] = cross_scores[idx]\n",
        "\n",
        "\n",
        "    #Output of top-10 hits\n",
        "    print(\"Top-\" + str(top_n_res) + \"Bi-Encoder Retrieval hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "    bi_encoder_output = []\n",
        "    for hit in hits[0:top_n_res]:\n",
        "        line_bi = str(round(hit['score'], 2)) + \" , \" + passages[hit['corpus_id']] #+ \" . \" + hit['corpus_id']\n",
        "        bi_encoder_output.append(line_bi)\n",
        "\n",
        "    \n",
        "    print(\"Top-\" + str(top_n_res) + \"Cross-Encoder Re-ranker hits\")\n",
        "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    cross_encoder_output = []\n",
        "    for hit in hits[0:top_n_res]:\n",
        "        line_c = str(round(hit['cross-score'], 2)) + \" , \" + passages[hit['corpus_id']]  + \" . \" + \"Åtgärder: \" + df.Åtgärdsbeskrivning[hit['corpus_id']] + \"\\n  \"\n",
        "        cross_encoder_output.append(line_c)\n",
        "    \n",
        "    return bm25_output, bi_encoder_output, cross_encoder_output\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_search,\n",
        "    inputs=[\"text\", \"text\", \"text\", \"number\", \"number\"], \n",
        "    outputs=[\"text\", \"text\", \"text\"])\n",
        "iface.launch(debug=True)"
      ],
      "id": "h_qP29AH6rgX",
      "execution_count": null,
      "outputs": []
    }
  ]
}