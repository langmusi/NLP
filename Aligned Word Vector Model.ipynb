{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/aligned-vectors.html  \n",
    "Aligning the fastText vectors of 78 languages https://github.com/babylonhealth/fastText_multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_dir = './Python Codes'\n",
    "import os\n",
    "os.chdir(python_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fasttext\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import Import_and_clean_data as ic\n",
    "import WCS_Clustering_MainFunction as cmf\n",
    "import SlimWeightedVecData as swd\n",
    "import OutputWarranty as ow\n",
    "import WarrantyCluster as wc\n",
    "import PreprocessText as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import FastVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FastVector:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, only used for export(), so that more frequent words are earlier in the file\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "            (self.n_words, self.n_dim) = \\\n",
    "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "\n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "\n",
    "    def apply_transform(self, transform):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        self.embed = np.matmul(self.embed, transmat)\n",
    "\n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing multiple outputs in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute instance = data-sci-experiment\n",
    "df_path = '~/cloudfiles/code/Users/cla.Min.Liu/OTU_SAP/Data/df_ny_gl_system_1_azure.csv'\n",
    "stopword_path = \"~/cloudfiles/code/Users/cla.Min.Liu/OTU_SAP/Data/danska-stopwords.csv\"\n",
    "#fasttext_link = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/data-sci-experiment/code/Users/cla.Min.Liu/cc.da.300.bin'\n",
    "word_vec_da = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/data-sci-experiment/code/Users/cla.Min.Liu/wiki.da.align.vec'\n",
    "word_vec_sv = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/data-sci-experiment/code/Users/cla.Min.Liu/wiki.sv.align.vec'\n",
    "#fasttext_link = \"wiki.da.align.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 200000 most common words for English (limit for loading time)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word_vec_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading embeddings\n",
    "da_dictionary = FastVector(vector_file = word_vec_da)\n",
    "sv_dictionary = FastVector(vector_file = word_vec_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = da_dictionary[\"køre\"]\n",
    "word_2 = sv_dictionary[\"kör\"]\n",
    "word_1.shape\n",
    "word_2.shape\n",
    "FastVector.cosine_similarity(word_1, word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_align(emb1, emb2, w1, w2):\n",
    "    w_da = emb1[w1]\n",
    "    w_es = smb2[w2]\n",
    "    print(FastVector.cosine_similarity(w_en, w_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ic.ImportAndCleanCSV(df_path, datenc = \"utf-8\", text_multi_var = True, text_var = \"Beskrivelse\")\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_word_vector(\"slitage\")   #fasttext\n",
    "#model['slitage']  # aligned model\n",
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbor queries  \n",
    "\n",
    "A simple way to check the quality of a word vector is to look at its nearest neighbors. This give an intuition of the type of semantic information the vectors are able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_nearest_neighbors('elg')\n",
    "model.most_similar('körd')  # kørt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = ic.ImportAndCleanCSV(df_path, datenc = \"utf-8\", text_multi_var = 2, text_var = \"Beskrivelse\")\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "def PreprocessData(dat, textcol, stopwordpath, stopenc):\n",
    "\n",
    "    # Import stopwords\n",
    "    stop_words = pd.read_csv(stopwordpath, encoding = stopenc, sep = \" \")\n",
    "    stopwords_custom = stop_words['stoppord'].values.tolist()\n",
    "  \n",
    "    stopwords_swe = stopwords.words('swedish')\n",
    "    stopwords_dan = stopwords.words('danish')\n",
    "\n",
    "    stop_words = itertools.chain(stopwords_swe, stopwords_dan, stopwords_custom)\n",
    "    stop_words = list(stop_words)\n",
    "\n",
    "    # Process text\n",
    "    dat['Text'] = [pt.PreprocessText(t, stop_words) for t in dat[textcol]]\n",
    "\n",
    "    return dat\n",
    "\n",
    "df = PreprocessData(df, textcol = 'Text', stopwordpath = stopword_path, stopenc = \"ISO 8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "def CalculateWeightedSentenceVector(text, fasttextmodel):\n",
    "    response, words = TF_Idf_WeightMatrix(text)\n",
    "    embmat = EmbeddingMatrix(words, fasttextmodel)\n",
    "\n",
    "    w_sentence_vec = response*embmat\n",
    "    return w_sentence_vec\n",
    "\n",
    "\n",
    "def CreateCorpus(textcol):\n",
    "    corpus = textcol.tolist()\n",
    "    return corpus\n",
    "\n",
    "def EmbeddingMatrix(words, fasttextmodel):\n",
    "    return np.array([fasttextmodel.vectors(w) for w in words])\n",
    "\n",
    "\n",
    "def TF_Idf_WeightMatrix(text):\n",
    "    corpus = CreateCorpus(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    response = vectorizer.fit_transform(corpus)\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    return response, words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words\n",
    "#test = CalculateWeightedSentenceVector(df['Text'], fasttextmodel = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PreprocessText as pt\n",
    "import WCS_tfidf as tfidf\n",
    "import fasttext\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "\n",
    "stop_words = pd.read_csv(stopword_path, encoding = \"ISO 8859-1\", sep = \" \")\n",
    "stopwords_custom = stop_words['stoppord'].values.tolist()\n",
    "stopwords_swe = stopwords.words('swedish')\n",
    "stopwords_dan = stopwords.words('danish')\n",
    "\n",
    "stop_words = itertools.chain(stopwords_swe, stopwords_dan, stopwords_custom)\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "# Process text\n",
    "df['Text'] = [pt.PreprocessText(t, stop_words) for t in df[\"Text\"]]\n",
    "\n",
    "response, words = TF_Idf_WeightMatrix(text = df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embmat = model.vectors\n",
    "response.shape\n",
    "#w_sentence_vec = response*embmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PreprocessText as pt\n",
    "import WCS_tfidf as tfidf\n",
    "import fasttext\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import gensim\n",
    "\n",
    "def WeightedSentenceVector(dat, textcol, stopwordpath, stopenc, fasttextmodel):\n",
    "    # Returns a weighted sentence vector\n",
    "\n",
    "    # Fasttextmodel\n",
    "    #model = fasttext.load_model(fasttextmodel)\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(fasttextmodel)\n",
    "    \n",
    "    # Import stopwords\n",
    "    stop_words = pd.read_csv(stopwordpath, encoding = stopenc, sep = \" \")\n",
    "    stopwords_custom = stop_words['stoppord'].values.tolist()\n",
    "  \n",
    "    stopwords_swe = stopwords.words('swedish')\n",
    "    stopwords_dan = stopwords.words('danish')\n",
    "\n",
    "    stop_words = itertools.chain(stopwords_swe, stopwords_dan, stopwords_custom)\n",
    "    stop_words = list(stop_words)\n",
    "\n",
    "    # Process text\n",
    "    dat['Text'] = [pt.PreprocessText(t, stop_words) for t in dat[textcol]]\n",
    "\n",
    "     # Create weighted word vector\n",
    "    WeightVec = CalculateWeightedSentenceVector(text = dat['Text'],\n",
    "                                                      fasttextmodel = model)\n",
    "\n",
    "    dat['WeightVec'] = [r for r in WeightVec]\n",
    "\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydat = WeightedSentenceVector(df, textcol = \"Text\", stopwordpath = stopword_path, \n",
    "                               stopenc = \"ISO 8859-1\",\n",
    "                               fasttextmodel = fasttext_link)\n",
    "mydat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining data for clustering\n",
    "\n",
    "The function DataForClustering is applied to the subset of the data (in the following example, the subset of the data is the one which system = 1.   \n",
    "\n",
    "The output of the function returns weighted sentence vectors, and index created for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = swd.DataForClustering(dat = df, group = False, \n",
    "                                      sort_var_1 = 'System', sort_var_2 = 'Meddelelsesdato',\n",
    "                                      textcol = 'Text',\n",
    "                                      stopwordpath = stopword_path, stopenc = \"ISO 8859-1\",\n",
    "                                      fasttext_link = fasttext_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp.head()\n",
    "df_nlp['Text'][0]  # the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_system.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/processed_df.csv', \n",
    "              sep=';', encoding='ISO 8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(dat_group.Cluster.loc[dat_group['AntalSimInds'] >= numdefects]) this removes small clusters that lie within the bigger ones\n",
    "sim_cluster_output = cmf.WCSClustering(df_nlp, fasttext_link,\n",
    "                                       group = False, groupvar = 'System',\n",
    "                                       unique_cluster = True, cos_similarity = 0.9,\n",
    "                                       numdefects = 2)\n",
    "sim_cluster_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from itertools import compress, chain\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "def OutputWarranty(dat, wordvec, textvec, simil, group, groupvar, timevar):\n",
    "    if group:\n",
    "        allout = dat.groupby(groupvar).apply(func = lambda s:\n",
    "                 FindSimsMatrix(s, similarity = simil, wordvec = wordvec, textvec = textvec,\n",
    "                                groupvar = groupvar, timevar = timevar))\n",
    "    else:\n",
    "        allout = FindSimsMatrix(dat, similarity = simil, wordvec = wordvec, textvec = textvec,\n",
    "                                groupvar = groupvar, timevar = timevar)\n",
    "\n",
    "    # Select only defects with <timethreshold> time inbetween\n",
    "    #FindWarranty(d = allout, timevar = timevar)\n",
    "    allout = SelectWarrantyRows(allout)\n",
    "\n",
    "    # Output: Select time period for output\n",
    "    ints = FindIndices(allout, simind = 'SimInd')\n",
    "    #allout = allout.loc[ints,:] # deprecated method\n",
    "    #allout = allout[allout['Index'].isin(ints)]\n",
    "    allout = allout.loc[allout.index.intersection(ints)] # chosen method\n",
    "    #allout['SimScore'] = [[round(elem, 2) for elem in l] for l in allout['SimScore']]\n",
    "    \n",
    "    allout['AntalSimInds'] = [len(l) for l in allout['SimInd']]\n",
    "\n",
    "    return allout\n",
    "\n",
    "def FindSimsMatrix(g, similarity, wordvec, textvec, groupvar, timevar):\n",
    "    vec = g[wordvec].tolist()\n",
    "    sim = CompareBackwardsMatrix(vec)\n",
    "    #simindex = [g['Index'][np.where(sim[row] > similarity)[0]].tolist() for row in range(sim.shape[0])]\n",
    "    #simindex = [g['Index'][FindSimIndex(row, similarity)].tolist() for row in sim]\n",
    "    simindex = [g['Index'][row > similarity].tolist() for row in sim]\n",
    "    simval = [FindSimValue(row, similarity) for row in sim]\n",
    "    return pandas.DataFrame({'SimScore': simval,\n",
    "                             'SimInd': simindex,\n",
    "                             textvec: g[textvec],\n",
    "                             wordvec: g[wordvec],\n",
    "                             'Index': g['Index'],\n",
    "                             groupvar: g[groupvar],\n",
    "                             timevar: g[timevar],\n",
    "                             'Meddelelse': g['Meddelelse']})\n",
    "\n",
    "# vec = g[wordvec].tolist() where g = data, wordvec = 'WeightVec'\n",
    "def CompareBackwardsMatrix(vec, metric='cosine'):\n",
    "    \"\"\"\n",
    "    It should compare backwards:\n",
    "    >>> vec = [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
    "    >>> res = CompareBackwardsMatrix(vec, metric='cosine')\n",
    "    >>> res\n",
    "    array([[0., 0.],\n",
    "           [1., 0.]])\n",
    "    \"\"\"\n",
    "    m = np.matrix(vec)\n",
    "    d = 1 - pairwise_distances(m, metric=metric)\n",
    "    return np.tril(d, -1)\n",
    "\n",
    "def SelectWarrantyRows(d):\n",
    "    keep = d.SimInd.astype(bool)\n",
    "    # for i in range(len(d)):\n",
    "    #     for s in d.SimInd[i]:\n",
    "    #         keep[s] = True\n",
    "    for inds in d.SimInd:\n",
    "        for s in inds:\n",
    "            keep[s] = True\n",
    "    return d[keep]\n",
    "\n",
    "def FindSimValue(sim, similarity):\n",
    "    index = sim > similarity\n",
    "    return sim[index]\n",
    "\n",
    "def FindSimIndex(sim, similarity):\n",
    "    return np.where(sim > similarity)[0]\n",
    "\n",
    "def FindIndices(d, simind):\n",
    "    #ints1 = [i for inds in d[simind] for i in inds]\n",
    "    ints1 = d[simind].values.tolist()\n",
    "    ints1 = list(chain.from_iterable(ints1))\n",
    "    ints2 = d['Index'].values.tolist()\n",
    "    ints = [*ints1, *ints2]\n",
    "    ints = list(set(ints))\n",
    "    ints.sort()\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity = 0.8\n",
    "similarity_output = OutputWarranty(dat = df_nlp_system,\n",
    "                                  wordvec = 'WeightVec',\n",
    "                                 textvec = 'Text', simil = cos_similarity,\n",
    "                                 group = True, groupvar = 'System',\n",
    "                                 timevar = 'Meddelelsesdato')\n",
    "similarity_output.shape\n",
    "similarity_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fleet = pd.merge(similarity_output, df_nlp_system[[\"Index\", \"Tågset\", \"Vagn\", \n",
    "                                            'System_text', 'Systemstatus', 'Beskrivelse']],\n",
    "                                             on=\"Index\", how='left')\n",
    "out_fleet['AntalSimInds'] = [len(l) for l in out_fleet['SimInd']]\n",
    "out_fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress, chain\n",
    "\n",
    "def WarrantyCluster(dat):\n",
    "    siminds = dat.SimInd.tolist()\n",
    "    inds = dat.Index.values.tolist()\n",
    "\n",
    "    Cluster = AddCluster(inds, siminds)\n",
    "    dat['Cluster'] = Cluster\n",
    "\n",
    "    return dat\n",
    "\n",
    "def AddCluster(inds, siminds):\n",
    "    midres = MiddleCluster(inds, siminds)\n",
    "    next = NextStep(midres, inds)\n",
    "    clusters = AddItself(next, inds)\n",
    "    return clusters\n",
    "\n",
    "def MiddleCluster(inds, siminds):\n",
    "    # This function checks which clusters each ind belongs to\n",
    "    midclusters = []\n",
    "    for i in range(len(inds)):\n",
    "        midclusters.append([])\n",
    "        for sim in siminds[i]:\n",
    "            midclusters[inds.index(sim)].append(inds[i])\n",
    "    return midclusters\n",
    "\n",
    "def NextStep(midres, inds):\n",
    "    # This function removes all inds which are not clusters,\n",
    "    # because they themselves belong to another cluster\n",
    "    for i in reversed(range(len(inds))):\n",
    "        if len(midres[i]) > 0: # For non-empty clusterlists\n",
    "            for j in range(i):\n",
    "                if inds[i] in midres[j]: # If the index exists in clusterlist\n",
    "                    if len(midres[j]) > 1: # If there are more than one cluster in clusterlist\n",
    "                        midres[j].remove(inds[i]) # Remove index i from midres j\n",
    "    return midres\n",
    "\n",
    "def AddItself(next, inds):\n",
    "    # This function adds the index itself as a cluster,\n",
    "    # if the index exists in the list of clusters (next)\n",
    "    cl = set(chain(*next))\n",
    "    for i in cl:\n",
    "        next[inds.index(i)].append(i)\n",
    "    return next\n",
    "\n",
    "def test_itShouldAddClusterToEachSimInd():\n",
    "    inds =    [8 , 9, 11  , 13 , 14    , 22       , 25 , 33]\n",
    "    siminds = [[],[],[8,9],[11],[11,13],[11,13,14],[14],[14,25]]\n",
    "\n",
    "    midres = MiddleCluster(inds, siminds)\n",
    "    silver = [[11],[11],[13,14,22],[14,22],[22,25,33],[]  ,[33],[]]\n",
    "    assert(midres == silver)\n",
    "\n",
    "    next = NextStep(midres, inds)\n",
    "    nextgold = [[11],[11],[22]   ,[22]   ,[22,33]   ,[],[33],[]]\n",
    "    assert(next == nextgold)\n",
    "\n",
    "    res = AddItself(next, inds)\n",
    "    gold = [[11],[11],[22, 11],[22],[22,33],[22],[33],[33]]\n",
    "    assert(res == gold)\n",
    "\n",
    "def FindClusters(clusters):\n",
    "    inds = list(set(chain(clusters)))\n",
    "    inds.sort()\n",
    "\n",
    "    return inds\n",
    "\n",
    "\n",
    "def UnstackListColum(df, lst_col):\n",
    "    unstack_df = pd.DataFrame({\n",
    "                        col:np.repeat(df[col].values, df[lst_col].str.len())\n",
    "                        for col in df.columns.difference([lst_col])\n",
    "                        }).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns.tolist()]\n",
    "    return unstack_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fleet_cluster = wc.WarrantyCluster(out_fleet) #From WarrantyCluster\n",
    "out_fleet_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack clusters\n",
    "def UnstackListColum(df, lst_col):\n",
    "    unstack_df = pd.DataFrame({\n",
    "                        col:np.repeat(df[col].values, df[lst_col].str.len())\n",
    "                        for col in df.columns.difference([lst_col])\n",
    "                        }).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns.tolist()]\n",
    "    return unstack_df\n",
    "\n",
    "unst = UnstackListColum(df = out_fleet_cluster, lst_col = 'Cluster')\n",
    "len(unst)\n",
    "unst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows in example cluster\n",
    "dat_group = unst.groupby([\"Cluster\"])[\"AntalSimInds\"].count().reset_index()\n",
    "\n",
    "# Select all clusters where AntalSimInds >= 3\n",
    "numdefects = 1\n",
    "clustersmall = list(dat_group.Cluster.loc[dat_group['AntalSimInds'] >= numdefects])\n",
    "small = unst[unst['Cluster'].isin(clustersmall)]\n",
    "    \n",
    "len(small)\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sim_cluster_output)\n",
    "sim_cluster_output.sort_values(by=['Counts','Cluster'], ascending=False, inplace=True)\n",
    "sim_cluster_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cluster_output['Cluster'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_cluster_output.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/cosine_sim_cluster_output_1.csv',\n",
    "#                         sep = \";\", header = True, encoding = \"ISO 8859-1\")\n",
    "\n",
    "sim_cluster_output.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/cosine_sim_cluster_output_2.csv',\n",
    "                         sep = \";\", decimal = \",\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what the clusters are\n",
    "sim_cluster_output.groupby('Cluster')['Cluster'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking total number of rows in one cluster\n",
    "pd.Series(sim_cluster_output.Cluster).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output only last defect per cluster\n",
    "sim_cluster_output_last = sim_cluster_output.loc[sim_cluster_output.groupby('Cluster').Meddelelsesdato.idxmax()]\n",
    "sim_cluster_output_last.sort_values(by=['Counts'], ascending=False, inplace=True)\n",
    "len(sim_cluster_output_last)\n",
    "sim_cluster_output_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing the elements in one cluster\n",
    "text_sim_id_dic = dict(zip(similarity_output.Index, similarity_output.Text))\n",
    "#text_sim_id_dic\n",
    "keys = similarity_output.SimInd[7135]\n",
    "for key in keys:\n",
    "    text_sim_id_dic.get(key)\n",
    "#text_sim_id_dic[(111, 116)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Clusters\n",
    "Clustering is an unsupervised operation, and KMeans requires that we specify the number of clusters. \n",
    "\n",
    "One simple approach is to plot the SSE for a range of cluster sizes. We look for the \"elbow\" where the SSE begins to level off. MiniBatchKMeans introduces some noise so I raised the batch and init sizes higher. Unfortunately the regular Kmeans implementation is too slow. You'll notice different random states will generate different charts. Here I chose 14 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(w, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_clusters = MiniBatchKMeans(n_clusters=18, init_size=1024, batch_size=2048, random_state=20).fit_predict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_clusters = ClusterWordVec(w, num_clusts = 10, n_init = 20, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Cluster'] = assigned_clusters\n",
    "df_1.groupby('Cluster').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_wordfreq = WordFreqInClusters(df = df_1, num_top = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_1[df_1['Cluster'].values == 1]\n",
    "test['System'].unique()\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
