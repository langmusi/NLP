{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Ioana-P/pca_and_clustering_for_edu_purposes/blob/master/newsgroups_LSA.ipynb\n",
    "https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/  \n",
    "https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "\n",
    "BERTopic https://pypi.org/project/bertopic/, https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.transform bertopics needs Python > 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/nlp\"\n",
    "os.chdir(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import Import_and_clean_data as ic\n",
    "import WCS_Clustering_MainFunction as cmf\n",
    "import SlimWeightedVecData as swd\n",
    "import OutputWarranty as ow\n",
    "import WarrantyCluster as wc\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install bertopic\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing multiple outputs in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/data/df_ny_gl_system_1_azure.csv\"\n",
    "stopword_path = \"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/nlp/danska-stopwords.csv\"\n",
    "fasttext_link = \"cc.da.300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here Text-variable is created for analysis \n",
    "# # there are some rows without any system assigned\n",
    "# df = ic.ImportAndCleanCSV(df_path, datenc = \"utf-8\", text_multi_var = 2, text_var = \"Beskrivelse\")\n",
    "# df.shape\n",
    "# df.drop_duplicates(keep = False, inplace = True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "#from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, _ = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Category-Variable for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportAndCleanCSV(datapath, datenc, text_var, new_col):\n",
    "    # Import and clean data\n",
    "    wcsdata = pd.read_csv(datapath, encoding = datenc, sep = \";\")\n",
    "    wcsdata.fillna('', inplace=True)\n",
    "\n",
    "    # To datetime\n",
    "    wcsdata[\"Meddelelsesdato\"] = pd.to_datetime(wcsdata['Meddelelsesdato'], format='%Y-%m-%d')\n",
    "    wcsdata[\"Afslutningsdato\"] = pd.to_datetime(wcsdata['Afslutningsdato'], format='%Y-%m-%d')\n",
    "\n",
    "    # Combine DefectHeading, DefectDescription and DefectCodeName\n",
    "    wcsdata[text_var] = wcsdata[text_var].astype(str)\n",
    "    wcsdata[\"Subsystem\"] = wcsdata[\"Subsystem\"].astype(str)\n",
    "    wcsdata[new_col] = wcsdata[text_var] + \" \" + wcsdata[\"Subsystem\"] + \" \" + \\\n",
    "    wcsdata[\"System_text\"] + \" \" + wcsdata[\"Subsys_text\"] \n",
    "\n",
    "    return wcsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ImportAndCleanCSV(df_path, datenc = \"utf-8\", text_var = \"System\", new_col = \"Category\")\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Category\")[\"Category\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/output/category.csv',\n",
    "                         decimal = \",\", header = True, index = False, sep = \";\", encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data to two parts by \"Landskod\". Creating the new column \"Category\" from \"Text\".\n",
    "df_et = df[df[\"Landskod\"] == \"ET\"]\n",
    "# df_et.shape\n",
    "# df_et.loc[df[\"Landskod\"] == \"ET\", \"Category\"] = df[\"Text\"]\n",
    "df_et.shape\n",
    "df_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et.groupby(\"Category\")[\"Category\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "df_et[\"cat\"] = df_et[\"System\"] + df_et[\"Subsystem\"]\n",
    "# sns.distplot(df_et.groupby(\"System\").count())\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "g = sns.catplot(x = \"cat\", kind = \"count\", data = df_et, height=8.27, aspect=11.7/8.27)\n",
    "g.set_xticklabels(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isna(df_et[\"Category\"]))\n",
    "df_et[df_et[\"Category\"] == \" \"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating index for each category\n",
    "df_et['idx'] = df_et.groupby(\"Category\").ngroup()\n",
    "#df_et[df_et[\"idx\"] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et = df_et.sort_values(\"idx\")\n",
    "df_et_drop = df_et.drop(df_et[df_et[\"idx\"] == 0].index)\n",
    "df_et_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_et.groupby(\"Category\").Category.count()\n",
    "# df_et.groupby(\"idx\").Category.count()\n",
    "\n",
    "cat_idx_df =df_et[[\"Category\", \"idx\"]].sort_values(\"idx\")\n",
    "cat_idx_df.shape\n",
    "cat_idx_df.head(30)\n",
    "cat_idx_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/output/category_et.csv',\n",
    "                         decimal = \",\", header = True, index = False, sep = \";\", encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different landskod giving us different format of faults\n",
    "\n",
    "Lack of data for landskod == 86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Meddelelse\"]\n",
    "# df[df[\"Landskod\"] == \"ET\"][\"Meddelelse\"]\n",
    "# df[df[\"Landskod\"] == \"86\"][\"Meddelelse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing column \"Text\" with our analysis target\n",
    "df_et_drop[\"Text\"] = df_et_drop[\"Beskrivelse\"] + \" \" + df_et_drop[\"Langtekst\"]\n",
    "df_et_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization\n",
    "# df_et['Text_Cat'] = df_et.groupby('Category')['Text'].transform(lambda x: ','.join(x))\n",
    "# df_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #text_cat_df = df_et[[\"Category\", \"idx\", \"Text_Cat\"]].drop_duplicates(keep = False)\n",
    "# text_cat_df = pd.concat(df_et[\"Category\"], df_et[\"idx\"], df_et[\"Text_Cat\"]).unique()\n",
    "# text_cat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning Text\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "\n",
    "# Import stopwords\n",
    "stop_words = pd.read_csv(stopword_path, encoding = \"ISO 8859-1\", sep = \" \")\n",
    "stopwords_custom = stop_words['stoppord'].values.tolist()\n",
    "  \n",
    "stopwords_swe = stopwords.words('swedish')\n",
    "stopwords_dan = stopwords.words('danish')\n",
    "\n",
    "stop_words = itertools.chain(stopwords_swe, stopwords_dan, stopwords_custom)\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "    \n",
    "def PreprocessText(document, stopwords):\n",
    "    # Remove dates\n",
    "    document = RemoveDates(document)\n",
    "    \n",
    "     # Remove IP address\n",
    "    document = RemoveIP(document)\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'[^\\w]', ' ', str(document))\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = document.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    document = ' '.join(tokens)\n",
    "    \n",
    "    # Stemming words\n",
    "    stemmer = SnowballStemmer(\"swedish\")\n",
    "    document = stemmer.stem(document)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"danish\")\n",
    "    document = stemmer.stem(document)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    document = stemmer.stem(document)\n",
    "    \n",
    "    # Remove number when the string only consists of digits\n",
    "    #document = re.sub(r'\\d', '', document)    # not working when digits at the begining of strings re.sub(r' [0-9]+', '', document)\n",
    "\n",
    "    # Remove single letters and short words \n",
    "    #document = ' '.join( [w for w in document.split() if len(w)>2] )\n",
    "\n",
    "    return document\n",
    "\n",
    "def RemoveDates(doc):\n",
    "    doc = re.sub(\"[0-9]{2,4}.[0-9]{2}.[0-9]{2,4}\", \"\", doc)\n",
    "    return doc\n",
    "\n",
    "def RemoveIP(doc):\n",
    "    doc =  re.sub(r'[0-9]+(?:\\.[0-9]+){3}', '', doc) \n",
    "    return doc\n",
    "\n",
    "\n",
    "df_et_drop[\"Text\"] = [PreprocessText(t, stop_words) for t in df_et_drop[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et_drop.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining data for clustering\n",
    "\n",
    "The function DataForClustering is applied to the subset of the data (in the following example, the subset of the data is the one which system = 1.   \n",
    "\n",
    "The output of the function returns weighted sentence vectors, and index created for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et_clean = swd.DataForClustering(dat = df_et_drop, group = True, \n",
    "                                      sort_var_1 = 'idx', sort_var_2 = 'Meddelelsesdato',\n",
    "                                      textcol = 'Text',\n",
    "                                      stopwordpath = stopword_path, stopenc = \"ISO 8859-1\",\n",
    "                                      fasttext_link = fasttext_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a small sample to test clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction techniques that follows the same method as Singular Value Decomposition.\n",
    "\n",
    "This is effectively just a truncated singular value decomposition of a (very high-rank and sparse) document-term matrix, with only the  r= n_topics largest singular values preserved.\n",
    "\n",
    "LSA unsupervised, rests on two points:  \n",
    "1. The ditributional hypothesis, which states that words with similar meanings appear frequently together.  \n",
    "2. Singular Value Decomposition SVD.\n",
    "\n",
    "The words weight is focused in this kind of analysis.\n",
    "\n",
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "count_vectorizer = CountVectorizer(max_features=40000)\n",
    "cat_1 = df_et_clean[df_et_clean[\"idx\"] == 36]\n",
    "term_matrix = count_vectorizer.fit_transform(cat_1[\"Text\"])\n",
    "#lda.fit(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {'n_components': [5, 10, 15, 20], 'learning_decay': [.5, .7, .9]}\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(term_matrix)\n",
    "# Best model\n",
    "#best_lda_model = model.best_estimator_\n",
    "# model parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_param = output_topics_number(df = df_et_clean, topicModel = lda, textCol = \"Text\")\n",
    "count_vectorizer = CountVectorizer(max_features=40000)\n",
    "category_list = []\n",
    "best_number_topics = []\n",
    "# len(df_et_clean[\"idx\"].unique())\n",
    "for i in (df_et_clean[\"idx\"].unique()): \n",
    "    df = df_et_clean[df_et_clean[\"idx\"] == i]\n",
    "\n",
    "    search_params = {'n_components': [5, 10, 15, 20], 'learning_decay': [.5, .7, .9]}\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "    term_matrix = count_vectorizer.fit_transform(df[\"Text\"])\n",
    "    model.fit(term_matrix)\n",
    "    # Best model\n",
    "    #best_model = model.best_estimator_\n",
    "    \n",
    "    best_number_topics.append((model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_et_clean[\"idx\"].unique()\n",
    "# df_et_clean[\"idx\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_number_topics)\n",
    "with open('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/output/best_nr_topics.txt', \n",
    "          'w') as f:\n",
    "    f.write(str(best_number_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gensim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import corpora, models, similarities, utils, matutils\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1 = df_et_clean[df_et_clean[\"idx\"] == 42]\n",
    "\n",
    "\n",
    "# convert text to list for convenience\n",
    "text_list = cat_1[\"Text\"].tolist()\n",
    "\n",
    "# tokenization. get text in a format that gensim can turn into a dictionary and corpus\n",
    "texts = [ [word for word in document.split() ] for document in text_list]\n",
    "\n",
    "# Create a corpus from a list of texts - association word to numeric id\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda_gensim = models.LdaModel(corpus, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_gensim.show_topic(2)  \n",
    "#lda_gensim.get_topics()\n",
    "for topic_id in range(lda_gensim.num_topics):\n",
    "    topk = lda_gensim.show_topic(topic_id, 10)\n",
    "    topk_words = [ w for w, _ in topk ]\n",
    "    \n",
    "    print('{}: {}'.format(topic_id, ' '.join(topk_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_et_clean[\"idx\"].unique())\n",
    "df_et_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_list = []\n",
    "\n",
    "for idx in (df_et_clean[\"idx\"].unique()):  # df_et_clean[\"idx\"].unique()\n",
    "    \n",
    "    cat_idx = df_et_clean[df_et_clean[\"idx\"] == idx]\n",
    "    \n",
    "    # convert text to list for convenience\n",
    "    text_list = cat_idx[\"Text\"].tolist()\n",
    "\n",
    "    # get text in a format that gensim can turn into a dictionary and corpus\n",
    "    texts = [ [word for word in document.split() ] for document in text_list]\n",
    "\n",
    "    # Create a corpus from a list of texts\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Train the model on the corpus.\n",
    "    lda_gensim = models.LdaModel(corpus, id2word=dictionary, num_topics=5)\n",
    "    \n",
    "    for topic_id in range(lda_gensim.num_topics):\n",
    "        topk = lda_gensim.show_topic(topic_id, 10)\n",
    "        topk_words = [ w for w, _ in topk]\n",
    "        \n",
    "        topic_words_list.append('{} {}: {}'.format(idx, topic_id, ' '.join(topk_words)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et_clean[df_et_clean.idx == 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"topics\": pd.Series(topic_words_list)})\n",
    "# test_df[\"Topics\"] = re.sub(r\"(\\d) (\\d):\", \"\", str(test_df[\"topics\"]))\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1 = test_df[\"topics\"].str.extract(\"(\\d) (\\d)\")\n",
    "topics_df = pd.DataFrame({\"Category\": test_df_1.loc[:, 0],\n",
    "              \"Topics_index\": test_df_1.loc[:, 1],\n",
    "              \"Topics\": test_df[\"topics\"]})\n",
    "topics_df.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/output/topics_res.csv',\n",
    "                         decimal = \",\", header = True, index = False, sep = \";\", encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling model[corpus] only creates a wrapper around the old corpus document stream – actual \n",
    "# conversions are done on-the-fly, during document iteration.\n",
    "cat_1 = df_et_clean[df_et_clean[\"idx\"] == 42]\n",
    "\n",
    "\n",
    "# convert text to list for convenience\n",
    "text_list = cat_1[\"Text\"].tolist()\n",
    "\n",
    "# get text in a format that gensim can turn into a dictionary and corpus\n",
    "texts = [ [word for word in document.split() ] for document in text_list]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda_gensim = models.LdaModel(corpus, id2word=dictionary, num_topics=5)\n",
    "\n",
    "lda_gensim.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes= 150\n",
    "np.random.seed(1) # setting up random seed to get the same results\n",
    "ldamodel= LdaMulticore(corpus, \n",
    "                    id2word=dictionary, \n",
    "                    num_topics=5, \n",
    "#                   alpha='asymmetric', \n",
    "                    chunksize= 4000, \n",
    "                    batch= True,\n",
    "                    minimum_probability=0.001,\n",
    "                    iterations=350,\n",
    "                    passes=passes)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldamodel.show_topics(num_words=25, formatted=False)\n",
    "for topic_id in range(ldamodel.num_topics):\n",
    "    topk = lda_gensim.show_topic(topic_id, 10)\n",
    "    topk_words = [ w for w, _ in topk ]\n",
    "    \n",
    "    print('{}: {}'.format(topic_id, ' '.join(topk_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the main topic for each row:\n",
    "all_topics = ldamodel.get_document_topics(corpus)\n",
    "num_docs = len(all_topics)\n",
    "all_topics_csr= matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy= all_topics_csr.T.toarray()\n",
    "major_topic= [np.argmax(arr) for arr in all_topics_numpy]\n",
    "major_topic\n",
    "cat_1['major_lda_topic']= major_topic\n",
    "cat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "for idx in (df_et_clean[\"idx\"].unique()): \n",
    "    cat_idx = df_et_clean[df_et_clean[\"idx\"] == idx]\n",
    "    \n",
    "    # convert text to list for convenience\n",
    "    text_list = cat_idx[\"Text\"].tolist()\n",
    "\n",
    "    # get text in a format that gensim can turn into a dictionary and corpus\n",
    "    texts = [ [word for word in document.split() ] for document in text_list]\n",
    "\n",
    "    # Create a corpus from a list of texts\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Train the model on the corpus.\n",
    "    passes= 150\n",
    "    np.random.seed(1) # setting up random seed to get the same results\n",
    "    lda_gensim= LdaMulticore(corpus, \n",
    "                    id2word=dictionary, \n",
    "                    num_topics=5, \n",
    "#                   alpha='asymmetric', \n",
    "                    chunksize= 4000, \n",
    "                    batch= True,\n",
    "                    minimum_probability=0.001,\n",
    "                    iterations=350,\n",
    "                    passes=passes)    \n",
    "    \n",
    "    all_topics = lda_gensim.get_document_topics(corpus)\n",
    "    num_docs = len(all_topics)\n",
    "    all_topics_csr= matutils.corpus2csc(all_topics)\n",
    "    all_topics_numpy= all_topics_csr.T.toarray()\n",
    "\n",
    "    major_topic= [np.argmax(arr) for arr in all_topics_numpy]\n",
    "    cat_idx['major_lda_topic']= major_topic\n",
    "    \n",
    "    res = res.append(cat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape\n",
    "res[\"idx\"].unique()\n",
    "#df_et_clean[\"idx\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/output/res_withTopics.csv',\n",
    "                         decimal = \",\", header = True, index = False, sep = \";\", encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res.idx == 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "The LDA is based upon two general assumptions:\n",
    "\n",
    "- Documents that have similar words usually have the same topic  \n",
    "- Documents that have groups of words frequently occurring together usually have the same topic.\n",
    "\n",
    "Mathematically, the above two assumptions can be represented as:\n",
    "\n",
    "- Documents are probability distributions over latent topics  \n",
    "- Topics are probability distributions over words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply LDA, we need to create vocabulary of all the words in our data. Remember from the previous article, we could do so with the help of a count vectorizer. \n",
    "\n",
    "class sklearn.feature_extraction.text.CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
    "\n",
    "ngram_rangetuple (min_n, max_n), default=(1, 1)\n",
    "The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "In the previous section we used thee count vectorizer, but in this section we will use TFIDF vectorizer since NMF works with TFIDF. We will create a document term matrix with TFIDF. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_exdef sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))traction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "doc_term_matrix = tfidf_vect.fit_transform(cat_1['Text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the document term matrix is generated, we can create a probability matrix that contains probabilities of all the words in the vocabulary for all the topics. To do so, we can use the NMF class from the sklearn.decomposition module. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(doc_term_matrix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous section, let's randomly get 10 words from our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n",
    "    print(tfidf_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nr = []\n",
    "topic_con = []\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    topic_nr.append(i)\n",
    "    topic_con.append([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "len(topic_nr)\n",
    "len(topic_con )\n",
    "df_document_topic = pd.DataFrame(list(zip(topic_nr, topic_con)), columns=['topic_nr','topic_con'])\n",
    "df_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_et_clean[\"idx\"].unique())\n",
    "range(len(df_et_clean[\"idx\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_category(category_idx, doc = df_et_clean[\"idx\"]):\n",
    "    cat = df_et_clean[doc == category_idx]\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    doc_term_matrix = tfidf_vect.fit_transform(cat[\"Text\"].values.astype('U'))\n",
    "    nmf = NMF(n_components=5, random_state=42, max_iter = 1000)\n",
    "    nmf.fit(doc_term_matrix)\n",
    "    \n",
    "    topic_nr = []\n",
    "    topic_con = []\n",
    "    \n",
    "    for i,topic in enumerate(nmf.components_):\n",
    "        topic_nr.append(i)\n",
    "        topic_con.append([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "        df_document_topic = pd.DataFrame(list(zip(topic_nr, topic_con)), columns=['topic_nr','topic_con'])\n",
    "    \n",
    "    return df_document_topic\n",
    "\n",
    "for cat in range(3): # len(df_et_clean[\"idx\"].unique())\n",
    "    res = topic_model_category(category_idx = cat)\n",
    "    res = pd.concat([res])\n",
    "    #df_document_topic = pd.DataFrame(list(zip(topic_nr, topic_con)), columns=['topic_nr','topic_con'])\n",
    "\n",
    "# res = topic_model_category(2)\n",
    "# pd.concat([res])\n",
    "# res\n",
    "# res = df_et_clean.groupby(\"idx\").apply(lambda x: topic_model_category(category_idx = x))\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cluster_output_1.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/cosine_sim_cluster_allSystem_defNu2.csv',\n",
    "                         decimal = \",\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df_et_clean[ df_et_clean[\"idx\"] == 1]\n",
    "    \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "doc_term_matrix = tfidf_vect.fit_transform(cat[\"Text\"].values.astype('U'))\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(doc_term_matrix)\n",
    "    \n",
    "topic_nr = []\n",
    "topic_con = []\n",
    "    \n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    topic_nr.append(i)\n",
    "    topic_con.append([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    df_document_topic = pd.DataFrame(list(zip(topic_nr, topic_con)), columns=['topic_nr','topic_con'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# from time import sleep\n",
    "# from textblob.exceptions import NotTranslated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ett   snabbt TÅG, som kör jättefort : över hastighetsgränsen\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "text_dan = \"En hurtig TOG, der kører meget hurtigt: over hastighedsgrænsen\"\n",
    "blob_dan = TextBlob(text_dan)\n",
    "print(blob.tags)\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment)  # returns (polarity, subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_tagging(texts, nlp, allowed_postags=['NN', 'NNP']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_cluster_output.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/cosine_sim_cluster_output_1.csv',\n",
    "#                         sep = \";\", header = True, encoding = \"ISO 8859-1\")\n",
    "\n",
    "sim_cluster_output_1.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/cosine_sim_cluster_allSystem_defNu2.csv',\n",
    "                         decimal = \",\", header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
