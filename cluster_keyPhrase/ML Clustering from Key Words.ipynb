{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rtatman/forum-post-embeddings-clustering  \n",
    "https://www.kaggle.com/rtatman/yake-helper-funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pip install git+https://github.com/LIAAD/yake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/nlp\"\n",
    "os.chdir(path)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import Import_and_clean_data as ic\n",
    "import WCS_Clustering_MainFunction as cmf\n",
    "import SlimWeightedVecData as swd\n",
    "import OutputWarranty as ow\n",
    "import WarrantyCluster as wc\n",
    "import WordFreqInClusters as wfc\n",
    "\n",
    "import yake_helper_funcs as yhf   # custom function\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt, floor\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "import string\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing multiple outputs in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/data/df_ny_gl_system_1.csv\"\n",
    "stopword_path = \"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/nlp/danska-stopwords.csv\"\n",
    "fasttext_link = \"cc.da.300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here Text-variable is created for analysis \n",
    "# there are some rows without any system assigned\n",
    "df = ic.ImportAndCleanCSV(df_path, datenc = \"ISO 8859-1\", text_multi_var = True, text_var = \"Beskrivelse\")\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining data for clustering\n",
    "\n",
    "The function DataForClustering is applied to the subset of the data (in the following example, the subset of the data is the one which system = 1.   \n",
    "\n",
    "The output of the function returns weighted sentence vectors, and index created for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = swd.DataForClustering(dat = df, group = True, \n",
    "                                      sort_var_1 = 'System', sort_var_2 = 'Meddelelsesdato',\n",
    "                                      textcol = 'Text',\n",
    "                                      stopwordpath = stopword_path, stopenc = \"ISO 8859-1\",\n",
    "                                      fasttext_link = fasttext_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nlp.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/processed_df.csv', \n",
    "#              sep=';', encoding='ISO 8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extact keywords & tokenize\n",
    "#keywords = yhf.keywords_yake(sample_posts, )\n",
    "keywords_tokenized = yhf.tokenizing_after_YAKE(df_nlp['Text'])\n",
    "#keywords_tokenized\n",
    "keyword_sets = [set(row) for row in keywords_tokenized]\n",
    "len(keyword_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding = wsv.WeightedSentenceVector(dat = df_nlp,\n",
    "                                               textcol = textcol,\n",
    "                                               stopwordpath = stopwordpath,\n",
    "                                               stopenc = stopenc,\n",
    "                                               fasttextmodel = fasttext_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperprameters\n",
    "\n",
    "# number of clusters currently based on the square root of the # of rows in the data\n",
    "# number of posts\n",
    "num_of_posts = df_nlp_system.shape[0]\n",
    "num_of_posts\n",
    "\n",
    "# Number of clusters is square root of the # of posts (rounded down)\n",
    "number_clusters = math.floor(math.sqrt(num_of_posts))\n",
    "number_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering\n",
    "What does mean Spectral Clustering and where is it used:\n",
    "https://www.quora.com/What-are-the-advantages-of-spectral-clustering-over-k-means-clustering\n",
    "Spectral clustering: graph clustering\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_system.WeightVec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting sequence to array\n",
    "import itertools\n",
    "weightVec_array = np.array(list(itertools.zip_longest(*df_nlp_system.WeightVec, fillvalue=0))).T\n",
    "weightVec_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default k-means label assignment didn't work well\n",
    "from sklearn.cluster import SpectralClustering\n",
    "clustering = SpectralClustering(n_clusters=number_clusters, \n",
    "                                assign_labels=\"discretize\",\n",
    "                                n_neighbors=number_clusters).fit(weightVec_array)\n",
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating HTML report of clusters\n",
    "\n",
    "# get sample post info by #\n",
    "def get_post_info_by_cluster(number, \n",
    "                             data,\n",
    "                             cluster):\n",
    "    return(data[cluster.labels_ == number])\n",
    "\n",
    "df_nlp_system_sub = df_nlp_system.drop('WeightVec', 1)   # where 1 is the axis number (0 for rows and 1 for columns.)\n",
    "\n",
    "for i in range(number_clusters):\n",
    "    \n",
    "    print(f\"Cluster {i}:\\n\")\n",
    "    print(get_post_info_by_cluster(i, \n",
    "                                   data = df_nlp_system_sub,\n",
    "                                   cluster = clustering))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_system_sub['Cluster'] = clustering.labels_\n",
    "set(clustering.labels_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_system_sub['Cluster'] = clustering.labels_\n",
    "cluster_spectral = wfc.WordFreqInClusters(df = df_nlp_system_sub)  # num_top = 5 for frequency\n",
    "cluster_spectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate HTML report of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of posts/cluster\n",
    "cluster_counts = pd.Series(clustering.labels_).value_counts()\n",
    "cluster_counts\n",
    "# look at distrobution of cluster labels\n",
    "size_df = pd.Series(clustering.labels_).value_counts().to_frame()\n",
    "\n",
    "size_df = size_df.rename(columns={0: \"size\"})\n",
    "\n",
    "#size_df['characteristic_words'] = 0\n",
    "#size_df['characteristic_words'] = size_df['characteristic_words'].astype(object)\n",
    "\n",
    "size_df[\"Cluster\"] = size_df.index\n",
    "\n",
    "size_df['link_to_posts'] = \"\"\n",
    "\n",
    "for index, row in size_df.iterrows():\n",
    "    current_cluster_label = row[\"Cluster\"]\n",
    "    link_to_posts = (f'<a href=\"#anchor_{current_cluster_label}\">Link to posts</a>')\n",
    "    size_df.at[index,'link_to_posts'] = link_to_posts\n",
    "\n",
    "size_df = size_df.drop(index)\n",
    "size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_df = pd.merge(size_df, cluster_spectral, on='Cluster', how='left')\n",
    "#size_df = size_df.join(cluster_spectral)\n",
    "size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/spectral_cluster_report.html\", 'w', encoding=\"ISO 8859-1\") as file:\n",
    "    # file header\n",
    "    file.writelines('<meta charset=\"ISO 8859-1\">\\n')\n",
    "    \n",
    "    # add cluster info\n",
    "    file.write(size_df.drop([\"Cluster\"], axis=1).to_html(escape=False))\n",
    "    file.write(\"\\n\")\n",
    "    for i in range(number_clusters):\n",
    "        if i in cluster_spectral.index:\n",
    "            file.write(f'\\n<h2 id=\"anchor_{i}\">Cluster {i}:</h2>\\n')\n",
    "            cluster_info = get_post_info_by_cluster(i,\n",
    "                                                    data = df_nlp_system_sub,\n",
    "                                                    cluster = clustering)\n",
    "            file.write(cluster_info.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal Clusters\n",
    "Clustering is an unsupervised operation, and KMeans requires that we specify the number of clusters. \n",
    "\n",
    "One simple approach is to plot the SSE for a range of cluster sizes. We look for the \"elbow\" where the SSE begins to level off. MiniBatchKMeans introduces some noise so I raised the batch and init sizes higher. Unfortunately the regular Kmeans implementation is too slow. You'll notice different random states will generate different charts. Here I chose 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(weightVec_array, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_clusters = MiniBatchKMeans(n_clusters=10, \n",
    "                                    compute_labels=True,\n",
    "                                    init_size=1024, batch_size=2048, random_state=20).fit(weightVec_array)\n",
    "assigned_clusters.labels_\n",
    "\n",
    "# get sample post info by #\n",
    "def get_post_info_by_cluster(number, \n",
    "                             data,\n",
    "                             cluster):\n",
    "    return(data[assigned_clusters.labels_ == number])\n",
    "\n",
    "df_nlp_system_sub = df_nlp_system.drop('WeightVec', 1)   # where 1 is the axis number (0 for rows and 1 for columns.)\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    print(f\"Cluster {i}:\\n\")\n",
    "    print(get_post_info_by_cluster(i, \n",
    "                                   data = df_nlp_system_sub,\n",
    "                                   cluster = assigned_clusters))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of posts/cluster\n",
    "pd.Series(assigned_clusters.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(f\"Cluster {i}:\\n\")\n",
    "print(get_post_info_by_cluster(i, \n",
    "                                   data = df_nlp_system_sub,\n",
    "                                   cluster = assigned_clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cluster_df = get_post_info_by_cluster(i, \n",
    "                                             data = df_nlp_system_sub,\n",
    "                                             cluster = assigned_clusters)\n",
    "# kmeans_cluster_df.to_csv('C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/kmeans_cluster.csv', \n",
    "#               sep=';', encoding='ISO 8859-1')\n",
    "kmeans_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distrobution of cluster labels\n",
    "size_df = pd.Series(assigned_clusters.labels_).value_counts().to_frame()\n",
    "size_df = size_df.rename(columns={0: \"size\"})\n",
    "size_df['characteristic_words'] = 0\n",
    "size_df['characteristic_words'] = size_df['characteristic_words'].astype(object)\n",
    "size_df[\"cluster_label\"] = size_df.index\n",
    "size_df['link_to_posts'] = \"\"\n",
    "\n",
    "for index, row in size_df.iterrows():\n",
    "    current_cluster_label = row[\"cluster_label\"]\n",
    "    link_to_posts = (f'<a href=\"#anchor_{current_cluster_label}\">Link to posts</a>')\n",
    "    size_df.at[index,'link_to_posts'] = link_to_posts\n",
    "    \n",
    "size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_system_sub['Cluster'] = assigned_clusters.labels_\n",
    "cluster_kmeans = wfc.WordFreqInClusters(df = df_nlp_system_sub)  # num_top = 5 for frequency\n",
    "cluster_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_df = size_df.join(cluster_kmeans)\n",
    "size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write and save cluster information to a html-file\n",
    "with open(\"C:/Users/LIUM3478/OneDrive Corp/OneDrive - Atkins Ltd/Work_Atkins/otu sap wcs 2020 12/nlp/output/cluster_report.html\", 'w', encoding=\"ISO 8859-1\") as file:\n",
    "    # file header\n",
    "    file.writelines('<meta charset=\"ISO 8859-1\">\\n')\n",
    "    \n",
    "    # add cluster info\n",
    "    file.write(size_df.drop([\"cluster_label\"], axis=1).to_html(escape=False))\n",
    "    file.write(\"\\n\")\n",
    "    for i in range(10):\n",
    "        if i in cluster_kmeans.index:\n",
    "            file.write(f'\\n<h2 id=\"anchor_{i}\">Cluster {i}:</h2>\\n')\n",
    "            cluster_info = get_post_info_by_cluster(i,\n",
    "                                                    data = df_nlp_system_sub,\n",
    "                                                    cluster = clustering)\n",
    "            file.write(cluster_info.to_html(escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_clusters = ClusterWordVec(w, num_clusts = 10, n_init = 20, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Cluster'] = assigned_clusters\n",
    "df_1.groupby('Cluster').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_wordfreq = WordFreqInClusters(df = df_1, num_top = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_1[df_1['Cluster'].values == 1]\n",
    "test['System'].unique()\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
